{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdmF8G_CeCYF",
        "outputId": "8591b657-8b03-4066-9203-35794ad8721b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Importamos librerias necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Input, GRU\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "from keras import callbacks\n",
        "from pathlib import Path \n",
        "\n",
        "# En caso de que usemos Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cargamos el dataset\n",
        "raw_data = pd.read_csv('/content/drive/MyDrive/Projecto Marvik/Datos/TWTR.csv')\n",
        "# raw_data = pd.read_csv('/content/drive/MyDrive/Projecto Marvik/Datos/META.csv')\n",
        "# raw_data = pd.read_csv('/content/drive/MyDrive/Projecto Marvik/Datos/S&P500.csv') # sacado de NASDAQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xYUVZi6I8f5"
      },
      "outputs": [],
      "source": [
        "def select_model(raw_data, lag, gru_neurons, epochs, optimizer):\n",
        "\n",
        "  # Nos quedamos solamente con el valor de cierre de la accion\n",
        "  raw_data = raw_data[['Close']]\n",
        "\n",
        "  # Normalizamos el dataset\n",
        "  dataset = raw_data.values\n",
        "  dataset = dataset.astype('float32')\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  dataset = scaler.fit_transform(dataset)\n",
        "\n",
        "  # Funcion para crear el dataset que le vamos a pasar al modelo, teniendo en cuenta el lag\n",
        "  def create_dataset(dataset, lag):\n",
        "      dataX, dataY = np.zeros((len(dataset)-lag,lag)), np.zeros(len(dataset)-lag)\n",
        "      for i in range(len(dataset)-lag):\n",
        "          dataX[i,:] = dataset[i:i+lag,0]\n",
        "          dataY[i] = dataset[i+lag,0]\n",
        "      return dataX, dataY\n",
        "\n",
        "  # Ejecutamos la funcion de create_dataset y establecemos el lag\n",
        "  datasetX, datasetY = create_dataset(dataset, lag)\n",
        "\n",
        "  # Separamos el dataset en train y test\n",
        "  train_size = int(len(datasetX) * 0.67)\n",
        "  test_size = len(datasetX) - train_size\n",
        "  trainX, testX = datasetX[0:train_size,:], datasetX[train_size:len(dataset),:]\n",
        "  trainY, testY = datasetY[0:train_size], datasetY[train_size:len(dataset)]\n",
        "\n",
        "  # Agregamos una dimensión para poder pasarle luego al modelo\n",
        "  trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "  testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
        "\n",
        "  # Capas y armado del modelo\n",
        "  model = Sequential([GRU(gru_neurons, input_shape=(1, trainX.shape[2])),\n",
        "                      Dense(gru_neurons/2, activation='relu'),\n",
        "                      Dense(gru_neurons/2, activation='relu'),\n",
        "                      Dense(1)])\n",
        "\n",
        "  # Seleccion de metricas y entrenamiento del modelo\n",
        "  model.compile(loss='mean_squared_error', optimizer=optimizer, metrics= \"MeanSquaredError\")\n",
        "  \n",
        "  # Recupera los mejores weights que el modelo encontro en todas las corridas\n",
        "  earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10, restore_best_weights=True)\n",
        "  \n",
        "  model.fit(trainX, trainY, batch_size=1, epochs=epochs, verbose=1, validation_split=0.1, callbacks=[earlystopping])\n",
        "\n",
        "  score = model.evaluate(testX, testY, verbose = 1)\n",
        "\n",
        "  # Hacemos las predicciones\n",
        "  trainPredict = model.predict(trainX)\n",
        "  testPredict = model.predict(testX)\n",
        "\n",
        "  # Invertimos la normalizacion de los datos para poder calcular el RMSE\n",
        "  trainPredict = scaler.inverse_transform(trainPredict)\n",
        "  trainY = scaler.inverse_transform([trainY])\n",
        "  testPredict = scaler.inverse_transform(testPredict)\n",
        "  testY = scaler.inverse_transform([testY])\n",
        "\n",
        "  # Usamos el RMSE como métrica para evaluar la eficiencia del modelo\n",
        "  # Train Score\n",
        "  trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "  # Test Score\n",
        "  testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
        "\n",
        "  # Creamos nuevos datasets para graficar las predicciones\n",
        "  # Train\n",
        "  trainPredictPlot = np.empty_like(dataset)\n",
        "  trainPredictPlot[:, :] = np.nan\n",
        "  trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
        "\n",
        "  # Test\n",
        "  testPredictPlot = np.empty_like(dataset)\n",
        "  testPredictPlot[:, :] = np.nan\n",
        "  testPredictPlot[len(trainPredict)+lag:, :] = testPredict\n",
        "\n",
        "  return (trainScore, testScore, trainPredictPlot, testPredictPlot, model, scaler, optimizer, lag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wMG6YPI5Ay2D"
      },
      "outputs": [],
      "source": [
        "# Prueba con diferentes optimizers para hallar el optimo en base al RMSE\n",
        "raw_data = raw_data\n",
        "gru_neurons = 64\n",
        "epochs = 50\n",
        "lags = range(1,11)\n",
        "# optimizers = ['Adadelta','Adagrad','Adam','Adamax','Ftrl','Nadam','RMSprop','SGD']\n",
        "optimizers = ['Adam','Adamax','Nadam','RMSprop']\n",
        "\n",
        "RMSE_opt = [select_model(raw_data, lag, gru_neurons, epochs, optim) for optim in optimizers for lag in lags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-ah1CMU4GEhZ"
      },
      "outputs": [],
      "source": [
        "opt, lag_model, RMSE_test = [] , [] , []\n",
        "for i, j, k, l, m, n, o, p in RMSE_opt:\n",
        "  RMSE_test.append(j)\n",
        "  opt.append(o)\n",
        "  lag_model.append(p)\n",
        "\n",
        "results = pd.concat([ pd.Series(opt), pd.Series(lag_model), pd.Series(RMSE_test)], axis=1)\n",
        "results.rename(columns={0:'opt', 1:'lags', 2:'RMSE_test'}, inplace=True)\n",
        "results = pd.pivot(results, index='opt', values='RMSE_test', columns='lags')\n",
        "print(results)\n",
        "\n",
        "# Mejor RMSE obtenido en todos los modelos\n",
        "print('Mejor optimizer: ' + str(results.index[np.argmin(results.min(axis=1))]))\n",
        "print('Mejor numero de lags: ' + str(results.T.index[np.argmin(results.min(axis=0))]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "86AoKRiaAkt3"
      },
      "outputs": [],
      "source": [
        "# Guardamos tabla de resultados\n",
        "filepath = Path('/content/drive/MyDrive/Projecto Marvik/results.csv')  \n",
        "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
        "results.to_csv(filepath , sep=',' ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0N6bvcFKYIrd"
      },
      "outputs": [],
      "source": [
        "results.T.plot(legend=\"best_fit\", figsize=(12,5), xticks=range(1,len(lags)+1), \\\n",
        "               title='RMSE plot', marker='o', ylabel='RMSE')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4V_y1Gsse-UH"
      },
      "outputs": [],
      "source": [
        "# Generamos una funcion para utilizar todo el dataset en el entrenamiento\n",
        "def run_model(raw_data, lag, gru_neurons, epochs, optimizer):\n",
        "\n",
        "  # Nos quedamos solamente con el valor de cierre de la accion\n",
        "  raw_data = raw_data[['Close']]\n",
        "\n",
        "  # Normalizamos el dataset\n",
        "  dataset = raw_data.values\n",
        "  dataset = dataset.astype('float32')\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  dataset = scaler.fit_transform(dataset)\n",
        "\n",
        "  # Funcion para crear el dataset que le vamos a pasar al modelo, teniendo en cuenta el lag\n",
        "  def create_dataset(dataset, lag):\n",
        "      dataX, dataY = np.zeros((len(dataset)-lag,lag)), np.zeros(len(dataset)-lag)\n",
        "      for i in range(len(dataset)-lag):\n",
        "          dataX[i,:] = dataset[i:i+lag,0]\n",
        "          dataY[i] = dataset[i+lag,0]\n",
        "      return dataX, dataY\n",
        "\n",
        "  # Ejecutamos la funcion de create_dataset y establecemos el lag\n",
        "  datasetX, datasetY = create_dataset(dataset, lag)\n",
        "\n",
        "  # Separamos el dataset en train y test\n",
        "  train_size = int(len(datasetX))\n",
        "  \n",
        "  trainX = datasetX[0:train_size,:]\n",
        "  trainY = datasetY[0:train_size]\n",
        "\n",
        "  # Agregamos una dimensión para poder pasarle luego al modelo\n",
        "  trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
        "\n",
        "  # Capas y armado del modelo\n",
        "  model = Sequential([GRU(gru_neurons, input_shape=(1, trainX.shape[2])),\n",
        "                      Dense(gru_neurons/2, activation='relu'),\n",
        "                      Dense(gru_neurons/2, activation='relu'),\n",
        "                      Dense(1)])\n",
        "\n",
        "  # Seleccion de metricas y entrenamiento del modelo\n",
        "  model.compile(loss='mean_squared_error', optimizer=optimizer, metrics= \"MeanSquaredError\")\n",
        "  \n",
        "  # Recupera los mejores weights que el modelo encontro en todas las corridas\n",
        "  earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10, restore_best_weights=True)\n",
        "  \n",
        "  model.fit(trainX, trainY, batch_size=1, epochs=epochs, verbose=1, validation_split=0.1, callbacks=[earlystopping])\n",
        "\n",
        "  # Hacemos las predicciones\n",
        "  trainPredict = model.predict(trainX)\n",
        "\n",
        "  # Invertimos la normalizacion de los datos para poder calcular el RMSE\n",
        "  trainPredict = scaler.inverse_transform(trainPredict)\n",
        "  trainY = scaler.inverse_transform([trainY])\n",
        "\n",
        "  # Usamos el RMSE como métrica para evaluar la eficiencia del modelo\n",
        "  trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
        "\n",
        "  # Creamos nuevos datasets para graficar las predicciones\n",
        "  trainPredictPlot = np.empty_like(dataset)\n",
        "  trainPredictPlot[:, :] = np.nan\n",
        "  trainPredictPlot[lag:len(trainPredict)+lag, :] = trainPredict\n",
        "\n",
        "  return (trainScore, trainPredictPlot, model, scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ap8DF0CEqXtx"
      },
      "outputs": [],
      "source": [
        "# Luego de optimizar los parametros, entrenamos el modelos con todo el dataset\n",
        "raw_data = raw_data\n",
        "gru_neurons = 64\n",
        "epochs = 50\n",
        "best_optimizer = results.index[np.argmin(results.min(axis=1))]\n",
        "best_lag = results.T.index[np.argmin(results.min(axis=0))]\n",
        "\n",
        "# Corremos la funcion\n",
        "trainScore, trainPredictPlot, model, scaler = run_model(raw_data, best_lag, gru_neurons, epochs, best_optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tyqPo_QtiOFk"
      },
      "outputs": [],
      "source": [
        "# Graficamos los resultados del nuevo modelo\n",
        "plt.figure(figsize=(25,7))\n",
        "plt.plot(raw_data['Close'])\n",
        "plt.plot(trainPredictPlot[:])\n",
        "plt.legend(['Original Time Series','trainPredictPlot'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MQDHSUwROcNe"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mRK4ltvwqSNM"
      },
      "outputs": [],
      "source": [
        "def predict_values(days, trainPredict, scaler): \n",
        "  future_preds = trainPredictPlot[-best_lag:]\n",
        "\n",
        "  for i in range(1, days+1):\n",
        "    preds_scaled = scaler.transform(future_preds[-best_lag:,:])\n",
        "    pred_val = scaler.inverse_transform(model.predict(preds_scaled.reshape(1,-1,best_lag)))\n",
        "    future_preds = np.append(future_preds, pred_val, axis=0)\n",
        "\n",
        "  future_preds = future_preds[best_lag-1:]\n",
        "  return future_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V9C4GZOXq0vN"
      },
      "outputs": [],
      "source": [
        "future_preds = predict_values(10, trainPredictPlot, scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GGgBMS4feCqv"
      },
      "outputs": [],
      "source": [
        "future_preds_plot = np.empty((len(trainPredictPlot)+len(future_preds), 1))\n",
        "future_preds_plot[:, :] = np.nan\n",
        "future_preds_plot[len(trainPredictPlot):, :] = future_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MeAl4Y1LWGue"
      },
      "outputs": [],
      "source": [
        "# Graficamos los ultimos 100 dias del dataset mas la prediccion\n",
        "plt.figure(figsize=(20,7))\n",
        "plt.plot(trainPredictPlot[-100:])\n",
        "plt.plot(future_preds_plot[-100-len(future_preds)+1:])\n",
        "plt.legend(['trainPredictPlot','future_preds_plot'])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Time_Series_GRU",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}